{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Generic replay buffer for standard gym tasks\n",
    "class TaskBuffer(object):\n",
    "    def __init__(self, state_dim, action_dim, sf_dim, buffer_size, device):\n",
    "        self.max_size = int(buffer_size)\n",
    "        self.device = device\n",
    "\n",
    "        self.ptr = 0\n",
    "        self.crt_size = 0\n",
    "\n",
    "        self.state = np.zeros((self.max_size, state_dim))\n",
    "        self.action = np.zeros((self.max_size, action_dim))\n",
    "        self.next_state = np.zeros((self.max_size, state_dim))\n",
    "        self.reward = np.zeros((self.max_size, 1))\n",
    "        self.not_done = np.zeros((self.max_size, 1))\n",
    "        self.task = np.zeros((self.max_size, sf_dim))\n",
    "\n",
    "\n",
    "    def add(self, state, action, next_state, reward, done, task):\n",
    "        self.state[self.ptr] = state\n",
    "        self.action[self.ptr] = action\n",
    "        self.next_state[self.ptr] = next_state\n",
    "        self.reward[self.ptr] = reward\n",
    "        self.not_done[self.ptr] = 1. - done\n",
    "        self.task[self.ptr] = task\n",
    "\n",
    "        self.ptr = (self.ptr + 1) % self.max_size\n",
    "        self.crt_size = min(self.crt_size + 1, self.max_size)\n",
    "\n",
    "    def sample(self, batch_size=32):\n",
    "        ind = np.random.randint(0, self.crt_size, size=batch_size)\n",
    "        return (\n",
    "            torch.FloatTensor(self.state[ind]).to(self.device),\n",
    "            torch.LongTensor(self.action[ind]).to(self.device),\n",
    "            torch.FloatTensor(self.next_state[ind]).to(self.device),\n",
    "            torch.FloatTensor(self.reward[ind]).to(self.device),\n",
    "            torch.FloatTensor(self.not_done[ind]).to(self.device),\n",
    "            torch.FloatTensor(self.task[ind]).to(self.device),\n",
    "        )\n",
    "\n",
    "\n",
    "    def save(self, save_folder):\n",
    "        np.save(f\"{save_folder}_state.npy\", self.state[:self.crt_size])\n",
    "        np.save(f\"{save_folder}_action.npy\", self.action[:self.crt_size])\n",
    "        np.save(f\"{save_folder}_next_state.npy\", self.next_state[:self.crt_size])\n",
    "        np.save(f\"{save_folder}_reward.npy\", self.reward[:self.crt_size])\n",
    "        np.save(f\"{save_folder}_not_done.npy\", self.not_done[:self.crt_size])\n",
    "        np.save(f\"{save_folder}_task.npy\", self.task[:self.crt_size])\n",
    "        np.save(f\"{save_folder}_ptr.npy\", self.ptr)\n",
    "\n",
    "\n",
    "    def load(self, save_folder, size=-1):\n",
    "        reward_buffer = np.load(f\"{save_folder}_reward.npy\")\n",
    "\n",
    "        # Adjust crt_size if we're using a custom size\n",
    "        size = min(int(size), self.max_size) if size > 0 else self.max_size\n",
    "        self.crt_size = min(reward_buffer.shape[0], size)\n",
    "\n",
    "        self.state[:self.crt_size] = np.load(f\"{save_folder}_state.npy\")[:self.crt_size]\n",
    "        self.action[:self.crt_size] = np.load(f\"{save_folder}_action.npy\")[:self.crt_size]\n",
    "        self.next_state[:self.crt_size] = np.load(f\"{save_folder}_next_state.npy\")[:self.crt_size]\n",
    "        self.reward[:self.crt_size] = reward_buffer[:self.crt_size]\n",
    "        self.not_done[:self.crt_size] = np.load(f\"{save_folder}_not_done.npy\")[:self.crt_size]\n",
    "        self.task[:self.crt_size] = np.load(f\"{save_folder}_task.npy\")[:self.crt_size]\n",
    "        print(f\"Replay Buffer loaded with {self.crt_size} elements.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "\n",
    "from utils import RMS, PBE\n",
    "\n",
    "def weights_init_(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight, gain=1)\n",
    "        torch.nn.init.constant_(m.bias, 0)\n",
    "\n",
    "class GaussianPolicy(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim,\n",
    "        action_dim,\n",
    "        sf_dim=5,\n",
    "        hidden_dim=256,\n",
    "        action_scale=1.0,\n",
    "        action_bias=0.0,\n",
    "        max_log_std=2,\n",
    "        min_log_std=-20,\n",
    "        repr_noise=1e-6,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(state_dim+sf_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.mu = nn.Linear(hidden_dim, action_dim)\n",
    "        self.log_std = nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "        self.action_scale = action_scale\n",
    "        self.action_bias = action_bias\n",
    "        self.max_log_std = max_log_std\n",
    "        self.min_log_std = min_log_std\n",
    "        self.repr_noise = repr_noise\n",
    "\n",
    "        self.apply(weights_init_)\n",
    "\n",
    "    def forward(self, state, task):\n",
    "        cat = torch.cat([state, task], dim=1)\n",
    "        h = F.relu(self.fc1(cat))\n",
    "        h = F.relu(self.fc2(h))\n",
    "        mu = self.mu(h)\n",
    "        log_std = self.log_std(h)\n",
    "        log_std = torch.clamp(log_std, min=self.min_log_std, max=self.max_log_std)\n",
    "\n",
    "        return mu, log_std\n",
    "\n",
    "    def sample(self, state, task):\n",
    "        mean, log_std = self.forward(state, task)\n",
    "        std = log_std.exp()\n",
    "        normal = Normal(mean, std)\n",
    "        x_t = normal.rsample()  # for reparameterization trick (mean + std * N(0,1))\n",
    "        y_t = torch.tanh(x_t)\n",
    "        action = y_t * self.action_scale + self.action_bias\n",
    "        log_prob = normal.log_prob(x_t)\n",
    "\n",
    "        # Enforcing Action Bound\n",
    "        log_prob -= torch.log(self.action_scale * (1 - y_t.pow(2)) + self.repr_noise)\n",
    "        log_prob = log_prob.sum(1, keepdim=True)\n",
    "        mean = torch.tanh(mean) * self.action_scale + self.action_bias\n",
    "        return action, log_prob, mean\n",
    "\n",
    "    def act(self, state, task):\n",
    "        # print(state.shape, self.fc1)\n",
    "        mean, _ = self.forward(state, task)\n",
    "        return torch.tanh(mean) * self.action_scale + self.action_bias\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim,\n",
    "        action_dim,\n",
    "        sf_dim=5,\n",
    "        hidden_dim=256,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        def make_critic():\n",
    "            critic = nn.Sequential(\n",
    "                nn.Linear(state_dim + action_dim + sf_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, sf_dim),\n",
    "            )\n",
    "\n",
    "            return critic\n",
    "        \n",
    "        self.critic1 = make_critic()\n",
    "        self.critic2 = make_critic()\n",
    "        self.apply(weights_init_)\n",
    "    \n",
    "    def forward(self, state, action, task):\n",
    "        cat = torch.cat([state, action, task], dim=1)\n",
    "        SF1, SF2 = self.get_SF(state, action, task)\n",
    "        return (\n",
    "            torch.einsum('bi,bi->b', SF1, task).unsqueeze(-1),\n",
    "            torch.einsum('bi,bi->b', SF2, task).unsqueeze(-1),\n",
    "        )\n",
    "    \n",
    "    def get_SF(self, state, action, task):\n",
    "        cat = torch.cat([state, action, task], dim=1)\n",
    "        return (\n",
    "            self.critic1(cat).squeeze(-1),\n",
    "            self.critic2(cat).squeeze(-1)\n",
    "        )\n",
    "    \n",
    "class Phi(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim,\n",
    "        sf_dim=5,\n",
    "        hidden_dim=256,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, sf_dim)\n",
    "        \n",
    "    def forward(self, state, norm=True):\n",
    "        out = F.relu(self.fc1(state))\n",
    "        out = F.relu(self.fc2(out))\n",
    "        out = self.fc3(out)\n",
    "        if norm:\n",
    "            return F.normalize(out, dim=-1)\n",
    "        return out\n",
    "    \n",
    "class IdentityPhi(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(1, 1)\n",
    "        \n",
    "    def forward(self, state, norm=True):\n",
    "        if norm:\n",
    "            return F.normalize(state, dim=-1)\n",
    "        return state \n",
    "    \n",
    "    \n",
    "class SAC_APS(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        env,\n",
    "        # essential args for RL\n",
    "        rollout_model=None,\n",
    "        discount=0.99,\n",
    "        tau=5e-3,\n",
    "        # the following args are for networks\n",
    "        actor_lr=3e-4,\n",
    "        critic_lr=3e-4,\n",
    "        learn_alpha=True,\n",
    "        # flag for using aps or not\n",
    "        aps=False,\n",
    "        # the following args for aps\n",
    "        sf_dim=5,\n",
    "        phi_lr=3e-4,\n",
    "        task_lr=3e-4,\n",
    "        update_task_frequency=5,\n",
    "        knn_k=12,\n",
    "        knn_rms=True,\n",
    "        knn_avg=True,\n",
    "        knn_clip=1e-4,\n",
    "    ):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        self.env = env\n",
    "        self.discount = discount\n",
    "        self.tau = tau\n",
    "        self.state_dim = self.env.observation_space.shape[0]\n",
    "        self.action_dim = self.env.action_space.shape[0]\n",
    "        self.max_action = float(self.env.action_space.high[0])\n",
    "        \n",
    "        if not aps:\n",
    "            sf_dim = env.observation_space.shape[0]\n",
    "            self.sf_dim = sf_dim\n",
    "            self.phi = IdentityPhi()\n",
    "        else:\n",
    "            self.sf_dim = sf_dim\n",
    "            self.phi = Phi(\n",
    "                state_dim=self.state_dim,\n",
    "                sf_dim=self.sf_dim\n",
    "            ).to(self.device)\n",
    "        self.phi_opt = torch.optim.Adam(self.phi.parameters(), lr=phi_lr)\n",
    "\n",
    "        self.rollout_model = rollout_model\n",
    "        if rollout_model is not None:\n",
    "            self.model_based = True\n",
    "        else:\n",
    "            self.model_based = False\n",
    "\n",
    "        # set networks \n",
    "        self.actor = GaussianPolicy(\n",
    "            state_dim=self.state_dim,\n",
    "            action_dim=self.action_dim,\n",
    "            sf_dim=self.sf_dim,\n",
    "            action_scale=self.max_action,\n",
    "        ).to(self.device)\n",
    "        \n",
    "        self.critic = Critic(\n",
    "            state_dim=self.state_dim,\n",
    "            action_dim=self.action_dim,\n",
    "            sf_dim=self.sf_dim,\n",
    "        ).to(self.device)\n",
    "        self.critic_target = copy.deepcopy(self.critic)\n",
    "\n",
    "        self.actor_opt = torch.optim.Adam(self.actor.parameters(), lr=actor_lr)\n",
    "        self.critic_opt = torch.optim.Adam(self.critic.parameters(), lr=critic_lr)\n",
    "\n",
    "        self.learn_alpha = learn_alpha\n",
    "        if learn_alpha:\n",
    "            self.entropy_target = -np.prod(\n",
    "                    self.env.action_space.shape).item()\n",
    "            self.entropy_target = torch.tensor(self.entropy_target).to(self.device) # convert the numpy.ndarray to torch.tensor\n",
    "            self.log_alpha = torch.zeros(1, requires_grad=True, device=self.device)\n",
    "            self.alpha_opt = torch.optim.Adam(\n",
    "                [self.log_alpha],\n",
    "                lr=actor_lr,\n",
    "            )\n",
    "            \n",
    "        # the following is for aps \n",
    "        self.sf_dim = sf_dim\n",
    "        self.update_task_frequency = update_task_frequency\n",
    "        self.knn_k = knn_k\n",
    "        self.knn_rms = knn_rms\n",
    "        self.knn_avg = knn_avg\n",
    "        self.knn_clip = knn_clip\n",
    "        self.temp_task = None # temp tasks are set through function : set_task\n",
    "        \n",
    "        self.task_learned = False\n",
    "        self.task = torch.ones((1, self.sf_dim), requires_grad=True, device=self.device) # task is only for RL phase (supervised learning phase)\n",
    "        self.task_opt = torch.optim.Adam([self.task], lr=task_lr)\n",
    "        \n",
    "        self.RMS = RMS(self.device)\n",
    "        self.PBE = PBE(\n",
    "            rms=self.RMS,\n",
    "            knn_clip=self.knn_clip, \n",
    "            knn_k=self.knn_k, \n",
    "            knn_avg=self.knn_avg, \n",
    "            knn_rms=self.knn_rms,\n",
    "            device=self.device\n",
    "        )\n",
    "        \n",
    "        self.iterations = 0\n",
    "            \n",
    "    def create_empty_replay_buffer(self, buffer_size=int(1e6)):\n",
    "        return TaskBuffer(\n",
    "            state_dim=self.state_dim,\n",
    "            action_dim=self.action_dim,\n",
    "            sf_dim=self.sf_dim,\n",
    "            buffer_size=buffer_size,\n",
    "            device=self.device,\n",
    "        )\n",
    "\n",
    "    def set_rollout_model(self, rollout_model):\n",
    "        assert rollout_model is not None, 'Cannot set a none-type rollout model'\n",
    "        self.rollout_model = rollout_model\n",
    "        self.model_based = True\n",
    "        \n",
    "    def intrinsic_reward(self, task, next_state):\n",
    "        # maxent reward\n",
    "        with torch.no_grad():\n",
    "            state_representation = self.phi(next_state, norm=False)\n",
    "        bonus = self.PBE(state_representation)\n",
    "        ent_bonus = bonus.view(-1, 1)\n",
    "\n",
    "        # successor feature reward\n",
    "        state_representation = state_representation / torch.norm(state_representation, dim=1, keepdim=True)\n",
    "        sf_reward = torch.einsum(\"bi,bi->b\", task, state_representation).reshape(-1, 1)\n",
    "\n",
    "        return ent_bonus, sf_reward\n",
    "    \n",
    "    def maybe_set_task(self, fine_tune):\n",
    "        if fine_tune and self.task_learned:\n",
    "            return self.task\n",
    "        elif (self.iterations + 1) % self.update_task_frequency:\n",
    "            return self.set_task()\n",
    "        \n",
    "        return self.temp_task\n",
    "    \n",
    "    def set_task(self):\n",
    "        task = torch.randn(self.sf_dim).to(self.device)\n",
    "        task = task / torch.norm(task)\n",
    "        self.temp_task = task\n",
    "        return task\n",
    "\n",
    "    def select_action(self, state, task, deterministic=False):\n",
    "        with torch.no_grad():\n",
    "            state = torch.from_numpy(state.reshape(1, -1)).float().to(self.device)\n",
    "            task = task.float().view(1, task.shape[-1])\n",
    "            if deterministic:\n",
    "                action = self.actor.act(state, task)\n",
    "            else:\n",
    "                action, _, _ = self.actor.sample(state, task)\n",
    "        return action.squeeze(0).data.cpu().numpy()\n",
    "\n",
    "    def train(self, replay_buffer, batch_size, fine_tune):\n",
    "        state, action, next_state, reward, not_done, task = replay_buffer.sample(batch_size)\n",
    "        \n",
    "        if fine_tune:\n",
    "            reward = reward\n",
    "            # task = self.task.repeat(batch_size, 1) # use the regressed task\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                ent_bonus, sf_reward = self.intrinsic_reward(task, next_state)\n",
    "                aps_reward = ent_bonus + sf_reward\n",
    "                reward = aps_reward\n",
    "            # calculte Phi-loss\n",
    "            phi_loss = - torch.einsum(\"bi,bi->b\", self.phi(next_state), task).mean()\n",
    "            # update Phi and Encoder\n",
    "            self.phi_opt.zero_grad()\n",
    "            phi_loss.backward()\n",
    "            self.phi_opt.step()\n",
    "        \n",
    "        # first train the alpha\n",
    "        if self.learn_alpha:\n",
    "            new_action, log_prob, mean = self.actor.sample(state, task)\n",
    "            log_prob = log_prob.unsqueeze(-1)\n",
    "            alpha_loss = -(self.log_alpha * (log_prob + self.entropy_target).detach()).mean()\n",
    "            alpha = self.log_alpha.exp()\n",
    "        else:\n",
    "            alpha_loss = 0\n",
    "            alpha = 1\n",
    "        \n",
    "        \n",
    "        q_new_action = torch.min(\n",
    "            *self.critic(state, new_action, task)\n",
    "        )\n",
    "        actor_loss = (alpha*log_prob - q_new_action).mean()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_action, new_log_prob, next_mean = self.actor.sample(next_state, task)\n",
    "            new_log_prob = new_log_prob.unsqueeze(-1)\n",
    "            target_Q = torch.min(\n",
    "                *self.critic_target(next_state, next_action, task)\n",
    "            ) - alpha * new_log_prob\n",
    "            target_Q = reward + not_done * self.discount * target_Q\n",
    "        \n",
    "        Q1, Q2 = self.critic(state, action, task)\n",
    "        critic_loss = F.mse_loss(Q1, target_Q) + F.mse_loss(Q2, target_Q)\n",
    "\n",
    "        # update networks\n",
    "        if self.learn_alpha:\n",
    "            self.alpha_opt.zero_grad()\n",
    "            alpha_loss.backward()\n",
    "            self.alpha_opt.step()\n",
    "\n",
    "        self.actor_opt.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_opt.step()\n",
    "\n",
    "        self.critic_opt.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_opt.step()\n",
    "        \n",
    "        self.maybe_regress_task(replay_buffer, batch_size, epoch_size=1)\n",
    "\n",
    "        self.update_target_network()\n",
    "        \n",
    "        self.iterations += 1\n",
    "    \n",
    "    def maybe_regree_task(replay_buffer, batch_size, epoch_size=1):\n",
    "        if replay_buffer.size > 4096 and self.iterations % self.update_task_frequency == 0:\n",
    "            self.regress_task(replay_buffer, batch_size, epoch_size)\n",
    "\n",
    "    def regress_task(self, replay_buffer, batch_size=32, epoch_size=1):\n",
    "        # there are two options, one is using gradient descent, the other is using torch.linalg.lstsq\n",
    "        self.task_learned = True\n",
    "        for epoch in range(epoch_size):\n",
    "            state, action, next_state, reward, *_ = replay_buffer.sample(batch_size=batch_size)\n",
    "            with torch.no_grad():\n",
    "                representation = self.phi(next_state)\n",
    "                \n",
    "            # estimate the reward\n",
    "            estimated_reward = torch.einsum(\"bi,bi->b\", representation, self.task.repeat(batch_size, 1)) # 256 by 169, 1 by 169\n",
    "            \n",
    "            # update the task\n",
    "            task_loss = F.mse_loss(reward, estimated_reward)\n",
    "            self.task_opt.zero_grad()\n",
    "            task_loss.backward()\n",
    "            self.task_opt.step()\n",
    "\n",
    "    def learn(self, replay_buffer, step_size, expl_step_size, fine_tune=False, batch_size=32, eval_freq=int(5e3)):\n",
    "        # make aliases\n",
    "        env = self.env\n",
    "        state = env.reset()\n",
    "        evaluations = []\n",
    "        episode_reward = 0\n",
    "        episode_timesteps = 0\n",
    "        episode_num = 0\n",
    "        expl_step_size = max(batch_size, expl_step_size)\n",
    "        task = self.set_task()\n",
    "        \n",
    "        if not fine_tune:\n",
    "            eval_freq *= 10\n",
    "\n",
    "        for step in tqdm(range(step_size)):\n",
    "            task = self.maybe_set_task(fine_tune)\n",
    "            episode_timesteps += 1  \n",
    "            # pick the action\n",
    "            if step < expl_step_size:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action = self.select_action(np.array(state), task, deterministic=False)\n",
    "            \n",
    "            # perform the action\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            done_bool = float(done) if episode_timesteps < env._max_episode_steps else 0\n",
    "            \n",
    "            # add the transition to the replay buffer\n",
    "            replay_buffer.add(state, action, next_state, reward, done_bool, task.cpu().data.numpy())\n",
    "            \n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            \n",
    "            if step >= expl_step_size:\n",
    "                self.train(replay_buffer, batch_size, fine_tune)\n",
    "            \n",
    "            if done: \n",
    "                # +1 to account for 0 indexing. +0 on ep_timesteps since it will increment +1 even if done=True\n",
    "                # print(f\"Total T: {step+1} Episode Num: {episode_num+1} Episode T: {episode_timesteps} Reward: {episode_reward:.3f}\")\n",
    "                # Reset environment\n",
    "                state, done = env.reset(), False\n",
    "                episode_reward = 0\n",
    "                episode_timesteps = 0\n",
    "                episode_num += 1\n",
    "            \n",
    "            if (step + 1) % eval_freq == 0:\n",
    "                evaluations.append(self.evaluate())\n",
    "                # np.save(f\"./results/{file_name}\", evaluations)\n",
    "                # if args.save_model: policy.save(f\"./models/{file_name}\")\n",
    "                \n",
    "        return evaluations\n",
    "\n",
    "    def evaluate(self, seed=0, eval_episodes=10):\n",
    "        eval_env = self.env\n",
    "\n",
    "        eval_env.seed(seed + 100)\n",
    "\n",
    "        avg_reward = 0.\n",
    "        for _ in range(eval_episodes):\n",
    "            state, done = eval_env.reset(), False\n",
    "            while not done:\n",
    "                action = self.select_action(np.array(state), self.task, deterministic=True)\n",
    "                # print(state.shape, np.array(state).shape)\n",
    "                next_state, reward, done, _ = eval_env.step(action)\n",
    "                # print(state.shape, next_state.shape)\n",
    "                state = next_state\n",
    "                avg_reward += reward\n",
    "\n",
    "        avg_reward /= eval_episodes\n",
    "\n",
    "        print(\"---------------------------------------\")\n",
    "        print(f\"Evaluation over {eval_episodes} episodes: {avg_reward:.3f}\")\n",
    "        print(\"---------------------------------------\")\n",
    "        return avg_reward\n",
    "    \n",
    "    def update_target_network(self):\n",
    "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "                target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('Hopper-v2')\n",
    "# env = gym.make('Hopper-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 9978/1000000 [00:03<06:08, 2684.82it/s]/home/mila/y/yiran.wang/.conda/envs/mujoco_env/lib/python3.7/site-packages/ipykernel_launcher.py:345: UserWarning: Using a target size (torch.Size([256, 256, 1])) that is different to the input size (torch.Size([256, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "/home/mila/y/yiran.wang/.conda/envs/mujoco_env/lib/python3.7/site-packages/ipykernel_launcher.py:378: UserWarning: Using a target size (torch.Size([256])) that is different to the input size (torch.Size([256, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  5%|▌         | 50008/1000000 [11:44<6:56:25, 38.02it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Evaluation over 10 episodes: 6.107\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 100008/1000000 [26:30<8:19:57, 30.00it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Evaluation over 10 episodes: 44.564\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 150006/1000000 [41:24<4:48:00, 49.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Evaluation over 10 episodes: 6.784\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 200005/1000000 [56:19<5:28:43, 40.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Evaluation over 10 episodes: 5.707\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 250009/1000000 [1:11:16<4:44:11, 43.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Evaluation over 10 episodes: 8.647\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 300010/1000000 [1:26:01<3:44:41, 51.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Evaluation over 10 episodes: 8.567\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 350011/1000000 [1:40:42<3:21:47, 53.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Evaluation over 10 episodes: 6.414\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 400007/1000000 [1:55:37<3:30:36, 47.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Evaluation over 10 episodes: 5.835\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 450009/1000000 [2:10:27<3:02:59, 50.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Evaluation over 10 episodes: 7.134\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 500009/1000000 [2:25:51<3:41:50, 37.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Evaluation over 10 episodes: 10.172\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 550005/1000000 [2:41:21<4:28:57, 27.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Evaluation over 10 episodes: 24.204\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 600010/1000000 [2:56:17<3:07:24, 35.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Evaluation over 10 episodes: 19.531\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 650006/1000000 [3:11:22<1:54:48, 50.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Evaluation over 10 episodes: 8.071\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 700005/1000000 [3:26:23<2:32:34, 32.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Evaluation over 10 episodes: 10.590\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 750007/1000000 [3:42:27<2:12:54, 31.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Evaluation over 10 episodes: 101.086\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 800008/1000000 [3:58:45<1:22:16, 40.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Evaluation over 10 episodes: 14.581\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 850005/1000000 [4:14:26<1:06:24, 37.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Evaluation over 10 episodes: 41.755\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 900010/1000000 [4:30:00<39:29, 42.20it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Evaluation over 10 episodes: 40.436\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 950007/1000000 [4:45:10<20:50, 39.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Evaluation over 10 episodes: 41.726\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000000/1000000 [5:00:15<00:00, 55.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Evaluation over 10 episodes: 44.102\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "aps = SAC_APS(\n",
    "    env,\n",
    "    sf_dim=5,\n",
    "    aps=True\n",
    ")\n",
    "replay_buffer = aps.create_empty_replay_buffer()\n",
    "evaluations = aps.learn(\n",
    "    replay_buffer,\n",
    "    step_size=int(1e6),\n",
    "    batch_size=256,\n",
    "    expl_step_size=int(1e4),\n",
    "    fine_tune=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 5413/100000 [00:01<00:32, 2871.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Evaluation over 10 episodes: 44.102\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 9622/100000 [00:02<00:23, 3831.18it/s]/home/mila/y/yiran.wang/.conda/envs/mujoco_env/lib/python3.7/site-packages/ipykernel_launcher.py:345: UserWarning: Using a target size (torch.Size([256, 256, 1])) that is different to the input size (torch.Size([256, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "/home/mila/y/yiran.wang/.conda/envs/mujoco_env/lib/python3.7/site-packages/ipykernel_launcher.py:378: UserWarning: Using a target size (torch.Size([256])) that is different to the input size (torch.Size([256, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      " 10%|█         | 10007/100000 [00:03<00:41, 2158.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Evaluation over 10 episodes: 44.102\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 15007/100000 [01:17<25:42, 55.11it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Evaluation over 10 episodes: 39.430\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 20013/100000 [02:26<25:22, 52.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Evaluation over 10 episodes: 39.397\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 25012/100000 [03:37<22:43, 54.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Evaluation over 10 episodes: 39.351\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 30013/100000 [04:51<20:17, 57.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Evaluation over 10 episodes: 39.573\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 35008/100000 [06:06<20:12, 53.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Evaluation over 10 episodes: 39.921\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 40012/100000 [07:17<18:23, 54.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Evaluation over 10 episodes: 39.693\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 45013/100000 [08:27<15:31, 59.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Evaluation over 10 episodes: 40.097\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 50009/100000 [09:45<17:14, 48.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Evaluation over 10 episodes: 39.906\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 55008/100000 [11:03<15:16, 49.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Evaluation over 10 episodes: 39.890\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 60011/100000 [12:18<13:36, 48.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Evaluation over 10 episodes: 39.882\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 65007/100000 [13:39<13:30, 43.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Evaluation over 10 episodes: 39.862\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 70012/100000 [15:00<09:48, 50.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Evaluation over 10 episodes: 40.047\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 75011/100000 [16:17<07:55, 52.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Evaluation over 10 episodes: 40.219\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 80006/100000 [17:34<07:50, 42.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Evaluation over 10 episodes: 40.236\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 85011/100000 [18:54<04:58, 50.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Evaluation over 10 episodes: 40.246\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 90011/100000 [20:12<03:25, 48.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Evaluation over 10 episodes: 40.237\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 95008/100000 [21:31<01:35, 52.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Evaluation over 10 episodes: 40.234\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [22:51<00:00, 72.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Evaluation over 10 episodes: 40.054\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ft_replay_buffer = aps.create_empty_replay_buffer()\n",
    "ft_evaluations = aps.learn(\n",
    "    ft_replay_buffer,\n",
    "    step_size=int(1e5),\n",
    "    batch_size=256,\n",
    "    expl_step_size=int(1e4),\n",
    "    fine_tune=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 5755/100000 [00:01<00:32, 2938.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Evaluation over 10 episodes: 40.054\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 9958/100000 [00:02<00:24, 3749.16it/s]/home/mila/y/yiran.wang/.conda/envs/mujoco_env/lib/python3.7/site-packages/ipykernel_launcher.py:345: UserWarning: Using a target size (torch.Size([256, 256, 1])) that is different to the input size (torch.Size([256, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "/home/mila/y/yiran.wang/.conda/envs/mujoco_env/lib/python3.7/site-packages/ipykernel_launcher.py:378: UserWarning: Using a target size (torch.Size([256])) that is different to the input size (torch.Size([256, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Evaluation over 10 episodes: 40.054\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 15007/100000 [01:11<26:52, 52.71it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Evaluation over 10 episodes: 40.046\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 20007/100000 [02:26<26:16, 50.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Evaluation over 10 episodes: 40.044\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 20153/100000 [02:28<09:50, 135.33it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_48147/1114184325.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mexpl_step_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mfine_tune\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m )\n",
      "\u001b[0;32m/tmp/ipykernel_48147/2264400477.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, replay_buffer, step_size, expl_step_size, fine_tune, batch_size, eval_freq)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mexpl_step_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfine_tune\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_48147/2264400477.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, replay_buffer, batch_size, fine_tune)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor_opt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m         \u001b[0mactor_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor_opt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/mujoco_env/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/mujoco_env/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ft_evaluations_2 = aps.learn(\n",
    "    ft_replay_buffer,\n",
    "    step_size=int(1e5),\n",
    "    batch_size=256,\n",
    "    expl_step_size=int(1e4),\n",
    "    fine_tune=True,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mujoco_env",
   "language": "python",
   "name": "mujoco_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
